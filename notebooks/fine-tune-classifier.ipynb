{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "da8588ef-60db-4f9c-a6de-e9f63d03129a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "e868df8d-0e2b-45d9-bf04-e88c8e98cd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(\"Using device: \" + device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "2edcfaa5-02a9-4cc4-b35f-e2ae6e2bc180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import os\n",
    "\n",
    "# Corpus and dictionary files to use\n",
    "corpus_file = 'democracy_reports_corpus.csv'\n",
    "corpus_file_url = \"https://github.com/backdem/democracy-datasets/raw/main/democracy_reports_corpus.csv\"\n",
    "\n",
    "# Download datsets if not already downloaded\n",
    "if not os.path.exists(corpus_file):\n",
    "    urllib.request.urlretrieve(corpus_file_url, corpus_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "44882512-1124-4aaa-9ee5-06306211f567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 sentence  \\\n",
      "403487  in particular, the oversight role to be perfor...   \n",
      "287463  there have been no such criminal cases in rece...   \n",
      "250805  greco concludes that recommendation xi has bee...   \n",
      "216381  it furthermore took note of a planned amendmen...   \n",
      "36625   During the year, civil society organizations (...   \n",
      "\n",
      "                                                  section  country  year  \\\n",
      "403487                                               none    italy  2011   \n",
      "287463                                               none   sweden  2013   \n",
      "250805                                               none  andorra  2020   \n",
      "216381                                               none  germany  2021   \n",
      "36625   ['Executive Summary', 'At a Glance', 'Assesses...  armenia  2022   \n",
      "\n",
      "                              source  \n",
      "403487                         greco  \n",
      "287463                         greco  \n",
      "250805                         greco  \n",
      "216381                         greco  \n",
      "36625   freedomhouse_nations-transit  \n"
     ]
    }
   ],
   "source": [
    "# Read csv file into Dataframe\n",
    "df = pd.read_csv(corpus_file, dtype={'year': str, 'sentence': str}, comment='#')\n",
    "# Print first row\n",
    "print(df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "ab078e26-33f7-4ac1-a5b4-04c3ac74effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random labels for calssification\n",
    "def generate_label_vector(x):\n",
    "    return np.random.randint(2, size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "ba308218-7f5b-4c6c-b0fb-07cf5f305555",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences_labels = pd.DataFrame(df['sentence'])\n",
    "# Insert string label column into DF\n",
    "df_sentences_labels.insert(loc=1,column=\"label_str\", value=df_labels)\n",
    "# Convert string label colum to numerical column\n",
    "#df_sentences_labels['label'] = pd.factorize(df_sentences_labels['label_str'])[0]\n",
    "df_sentences_labels['label'] = df_sentences_labels['label_str'].apply(generate_label_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "32fb08fd-7122-43a0-a264-25afc93a9d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>366301</th>\n",
       "      <td>it is answerable to the minister for the civil...</td>\n",
       "      <td>[0, 1, 1, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430509</th>\n",
       "      <td>the parliament of georgia is unicameral.</td>\n",
       "      <td>[0, 1, 0, 1, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10440</th>\n",
       "      <td>In 2020, about 16,000 Venezuelans resided on C...</td>\n",
       "      <td>[0, 0, 0, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286398</th>\n",
       "      <td>high-profile cases allegedly rarely reached th...</td>\n",
       "      <td>[0, 1, 0, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242515</th>\n",
       "      <td>greco welcomes that such a system is now in pl...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414926</th>\n",
       "      <td>police and sbgs officers may be dismissed for ...</td>\n",
       "      <td>[0, 1, 0, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123282</th>\n",
       "      <td>Meanwhile programs helped to stimulate the hou...</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238028</th>\n",
       "      <td>in the meantime, the newly elected president h...</td>\n",
       "      <td>[0, 1, 1, 1, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87589</th>\n",
       "      <td>For example, In April 2022, a court sentenced ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376945</th>\n",
       "      <td>les partis politiques doivent être enregistrés...</td>\n",
       "      <td>[0, 1, 1, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence               label\n",
       "366301  it is answerable to the minister for the civil...  [0, 1, 1, 0, 1, 0]\n",
       "430509           the parliament of georgia is unicameral.  [0, 1, 0, 1, 0, 1]\n",
       "10440   In 2020, about 16,000 Venezuelans resided on C...  [0, 0, 0, 1, 1, 0]\n",
       "286398  high-profile cases allegedly rarely reached th...  [0, 1, 0, 1, 1, 1]\n",
       "242515  greco welcomes that such a system is now in pl...  [1, 0, 1, 0, 1, 1]\n",
       "414926  police and sbgs officers may be dismissed for ...  [0, 1, 0, 1, 1, 1]\n",
       "123282  Meanwhile programs helped to stimulate the hou...  [0, 1, 0, 0, 1, 0]\n",
       "238028  in the meantime, the newly elected president h...  [0, 1, 1, 1, 0, 1]\n",
       "87589   For example, In April 2022, a court sentenced ...  [0, 1, 1, 0, 0, 1]\n",
       "376945  les partis politiques doivent être enregistrés...  [0, 1, 1, 1, 0, 0]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace df from here onwords\n",
    "new_df = df_sentences_labels\n",
    "# Display a sample of the dataset\n",
    "new_df.sample(10)[['sentence', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "b06f297b-13c1-4775-b201-bc4b0d92d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "caf982b0-ceba-4f6a-b82a-de77aa727c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.comment_text = dataframe.sentence\n",
    "        self.targets = self.data.label\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comment_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment_text = str(self.comment_text[index])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "aba08395-2d99-4a90-86e2-b9ae7126776c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (460728, 3)\n",
      "TRAIN Dataset: (46073, 3)\n",
      "TEST Dataset: (414655, 3)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.1\n",
    "train_dataset=new_df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=new_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "56ef5fc2-7e42-492f-b784-60af2502b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "63817466-1fa8-4374-98ae-754830ef222d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l2): Dropout(p=0.3, inplace=False)\n",
       "  (l3): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 6)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "976033ad-b99d-4064-afd8-c8b1735fff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "e4d1e80e-f264-49e5-9246-39c497e48c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "2a6fc239-7a30-447e-8aca-fe460c4ae3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        if _%5000==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc7deb-ef71-4496-a8ae-49419949da33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.7036323547363281\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92565cc2-618d-4769-baf0-511e9b6f4a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9fb40f-c281-4f5e-813e-b2939c519c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    outputs, targets = validation(epoch)\n",
    "    outputs = np.array(outputs) >= 0.5\n",
    "    accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "    print(f\"Accuracy Score = {accuracy}\")\n",
    "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "    print(f\"F1 Score (Macro) = {f1_score_macro}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
