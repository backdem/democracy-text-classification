{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da8588ef-60db-4f9c-a6de-e9f63d03129a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import pyarrow\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e868df8d-0e2b-45d9-bf04-e88c8e98cd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(\"Using device: \" + device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2edcfaa5-02a9-4cc4-b35f-e2ae6e2bc180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Corpus and dictionary files to use\n",
    "#corpus_file = 'democracy_reports_corpus.csv'\n",
    "corpus_file = '../../data/democracy_reports_corpus_annelisa_fixed.csv'\n",
    "#dictionary_file = 'dimension_dictionary.json'\n",
    "#corpus_file_url = \"https://github.com/backdem/democracy-datasets/raw/main/democracy_reports_corpus.csv\"\n",
    "#dictionary_file_url = \"https://raw.githubusercontent.com/backdem/democracy-datasets/main/dimension_dictionary.json\"\n",
    "\n",
    "# Download datsets if not already downloaded\n",
    "#if not os.path.exists(corpus_file):\n",
    "#    urllib.request.urlretrieve(corpus_file_url, corpus_file)\n",
    "#if not os.path.exists(dictionary_file):\n",
    "#    urllib.request.urlretrieve(dictionary_file_url, dictionary_file)\n",
    "\n",
    "#def load_json_dict(dict_file):\n",
    "#    with open(dict_file, 'r') as file:\n",
    "#        dictionary = json.load(file)\n",
    "#        dictionary.append({\n",
    "#           'name': 'no_dimension',\n",
    "#            'words': []\n",
    "#        })\n",
    "#        return dictionary\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44882512-1124-4aaa-9ee5-06306211f567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              sentence   country  year  \\\n",
      "192  \"The PP and its allies also took 130 directly ...     spain  2018   \n",
      "516  \"However, a significant portion of the general...  slovenia  2021   \n",
      "402  \"Several mass-casualty incidents were reported...   germany  2021   \n",
      "107  \"Police use force to break up unsanctioned pro...    turkey  2021   \n",
      "450  \"Parties winning less than 4 percent but more ...  slovenia  2021   \n",
      "\n",
      "                          source            dimension1 dimension2 dimension3  \\\n",
      "192  freedomhouse_freedom-world\"             electoral        NaN        NaN   \n",
      "516  freedomhouse_freedom-world\"  liberal institutions        NaN        NaN   \n",
      "402  freedomhouse_freedom-world\"             ambiguous        NaN        NaN   \n",
      "107  freedomhouse_freedom-world\"  liberal institutions        NaN        NaN   \n",
      "450  freedomhouse_freedom-world\"             electoral        NaN        NaN   \n",
      "\n",
      "    dimension4 backsliding cat_4_sentence_nuance  \\\n",
      "192        NaN           3                   NaN   \n",
      "516        NaN           2                   NaN   \n",
      "402        NaN           3                   NaN   \n",
      "107        NaN           3                   NaN   \n",
      "450        NaN           3                   NaN   \n",
      "\n",
      "                         comments undefined0  undefined1 consensus  \n",
      "192                           NaN        NaN         NaN       NaN  \n",
      "516  although improved, still bad        NaN         NaN       NaN  \n",
      "402                           NaN        NaN         NaN       NaN  \n",
      "107           feitelijke weergave        NaN         NaN       NaN  \n",
      "450                           NaN        NaN         NaN       NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pj/wj7m849s68qcg6_lr0j03jgh0000gn/T/ipykernel_40048/3569966420.py:2: DtypeWarning: Columns (4,5,6,7,8,9,10,11,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(corpus_file, dtype={'year': str},comment='#')\n"
     ]
    }
   ],
   "source": [
    "# Read csv file into Dataframe\n",
    "df = pd.read_csv(corpus_file, dtype={'year': str},comment='#')\n",
    "# Filter for labelled data\n",
    "df_labelled = df[(df['dimension1'].notnull()) | (df['dimension2'].notnull()) | (df['dimension3'].notnull()) | (df['dimension4'].notnull())] \n",
    "# Print sample rows\n",
    "print(df_labelled.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba308218-7f5b-4c6c-b0fb-07cf5f305555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found labels: ['electoral' 'liberal institutions' 'participatory' 'liberal rights'\n",
      " 'ambiguous' 'media']\n",
      "Fixed labels: ['ambiguous', 'electoral', 'liberal institutions', 'liberal rights', 'media', 'participatory']\n"
     ]
    }
   ],
   "source": [
    "# Get label names\n",
    "labels = df_labelled['dimension1'].unique()\n",
    "print(f'Found labels: {labels}')\n",
    "# Fix typos in labels\n",
    "def find_replace_in_column(df, column_name, string_to_match, new_value):\n",
    "    df.loc[df[column_name] == string_to_match, column_name] = new_value\n",
    "    return df\n",
    "df_labelled = find_replace_in_column(df_labelled, 'dimension1', 'media ', 'media')\n",
    "df_labelled = find_replace_in_column(df_labelled, 'dimension1', 'electoral?', 'electoral')\n",
    "df_labelled = find_replace_in_column(df_labelled, 'dimension1', 'liberal rights?', 'liberal rights')\n",
    "df_labelled = find_replace_in_column(df_labelled, 'dimension1', 'liberal rights ', 'liberal rights')\n",
    "df_labelled = find_replace_in_column(df_labelled, 'dimension1', 'liberal righ', 'liberal rights')\n",
    "df_labelled = find_replace_in_column(df_labelled, 'dimension1', 'media l', 'media')\n",
    "labels = sorted(df_labelled['dimension1'].unique())\n",
    "print(f'Fixed labels: {labels}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20e7301c-fe00-4c1c-aca3-020991c671a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              sentence   country  year  \\\n",
      "68   \"During the trial, Turkish-Iranian businessman...    turkey  2021   \n",
      "36   \"The government has also resorted to arresting...    turkey  2021   \n",
      "186  \"Either the PP or the PSOE have typically held...     spain  2018   \n",
      "481  \"Some critics assert that the change is aimed ...  slovenia  2021   \n",
      "169  \"Private property rights are legally enshrined...    turkey  2021   \n",
      "\n",
      "                          source            dimension1            dimension2  \\\n",
      "68   freedomhouse_freedom-world\"  liberal institutions                   NaN   \n",
      "36   freedomhouse_freedom-world\"             electoral                   NaN   \n",
      "186  freedomhouse_freedom-world\"             electoral                   NaN   \n",
      "481  freedomhouse_freedom-world\"                 media  liberal institutions   \n",
      "169  freedomhouse_freedom-world\"  liberal institutions                   NaN   \n",
      "\n",
      "    dimension3 dimension4 backsliding cat_4_sentence_nuance comments  \\\n",
      "68         NaN        NaN           2                   NaN      NaN   \n",
      "36         NaN        NaN           2                   NaN      NaN   \n",
      "186        NaN        NaN         NaN                   NaN        ?   \n",
      "481        NaN        NaN           2                   NaN      NaN   \n",
      "169        NaN        NaN           2                   NaN      NaN   \n",
      "\n",
      "    undefined0  undefined1 consensus        label_vector  \n",
      "68         NaN         NaN       NaN  [0, 0, 1, 0, 0, 0]  \n",
      "36         NaN         NaN       NaN  [0, 1, 0, 0, 0, 0]  \n",
      "186        NaN         NaN       NaN  [0, 1, 0, 0, 0, 0]  \n",
      "481        NaN         NaN       NaN  [0, 0, 0, 0, 1, 0]  \n",
      "169        NaN         NaN       NaN  [0, 0, 1, 0, 0, 0]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pj/wj7m849s68qcg6_lr0j03jgh0000gn/T/ipykernel_40048/1175420483.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_labelled['label_vector'] = df_labelled.apply(generate_label_vector, l=labels, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Create label vector column\n",
    "# Label sentences with topics, producing \n",
    "# a masked label vector of the form [0, 0, 0, 0, 0, 1] where the \n",
    "# indicies match topics ['ambiguous', 'electoral', 'liberal institutions', 'liberal rights', 'media', 'participatory']\n",
    "def generate_label_vector(row, l):\n",
    "    label = row['dimension1']\n",
    "    return [int(l == label) for l in labels]\n",
    "    \n",
    "\n",
    "#df_sentences_labels = pd.DataFrame(df['sentence'])\n",
    "df_labelled['label_vector'] = df_labelled.apply(generate_label_vector, l=labels, axis=1)\n",
    "print(df_labelled.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b06f297b-13c1-4775-b201-bc4b0d92d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "FRACTION_OF_DS_TO_USE = 1\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 1e-05\n",
    "MAX_LOSS = 0.02\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8340b0d-717f-4135-a422-a0d9d8e83059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>\"The constitutional right to organize in diffe...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>\"The Serb List has been accused of harassing r...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>\"Pandemic-related rules on social distancing w...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>\"The Committee to Protect Journalists reported...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>\"Legal safeguards to ensure government transpa...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>\"Germany is obligated to enhance legal protect...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>\"The legislature is composed of the 40-seat Na...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>\"The constitution gives all citizens age 18 or...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>\"Several mass-casualty incidents were reported...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>\"Elected officials are generally free to make ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence        label_vector\n",
       "448  \"The constitutional right to organize in diffe...  [0, 1, 0, 0, 0, 0]\n",
       "652  \"The Serb List has been accused of harassing r...  [0, 1, 0, 0, 0, 0]\n",
       "108  \"Pandemic-related rules on social distancing w...  [0, 0, 0, 0, 0, 1]\n",
       "82   \"The Committee to Protect Journalists reported...  [0, 0, 0, 0, 1, 0]\n",
       "234  \"Legal safeguards to ensure government transpa...  [0, 0, 1, 0, 0, 0]\n",
       "334  \"Germany is obligated to enhance legal protect...  [0, 0, 1, 0, 0, 0]\n",
       "441  \"The legislature is composed of the 40-seat Na...  [0, 1, 0, 0, 0, 0]\n",
       "316  \"The constitution gives all citizens age 18 or...  [0, 1, 0, 0, 0, 0]\n",
       "402  \"Several mass-casualty incidents were reported...  [1, 0, 0, 0, 0, 0]\n",
       "220  \"Elected officials are generally free to make ...  [0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use part of the dataset\n",
    "new_df = df_labelled.sample(frac=FRACTION_OF_DS_TO_USE, random_state=200)\n",
    "# Display a sample of the dataset\n",
    "new_df.sample(10)[['sentence', 'label_vector']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "caf982b0-ceba-4f6a-b82a-de77aa727c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.comment_text = dataframe.sentence\n",
    "        self.targets = self.data.label_vector\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comment_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment_text = str(self.comment_text[index])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aba08395-2d99-4a90-86e2-b9ae7126776c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (665, 15)\n",
      "TRAIN Dataset: (532, 15)\n",
      "TEST Dataset: (133, 15)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_dataset=new_df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=new_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56ef5fc2-7e42-492f-b784-60af2502b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63817466-1fa8-4374-98ae-754830ef222d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l2): Dropout(p=0.3, inplace=False)\n",
       "  (l3): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 6)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "976033ad-b99d-4064-afd8-c8b1735fff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4d1e80e-f264-49e5-9246-39c497e48c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a6fc239-7a30-447e-8aca-fe460c4ae3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        if _%500==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        if loss.item() < MAX_LOSS:\n",
    "            break\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc7deb-ef71-4496-a8ae-49419949da33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.6473258137702942\n",
      "Epoch 0 done\n",
      "Epoch: 1, Loss:  0.46624812483787537\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for epoch in range(EPOCHS):\n",
    "    loss = train(epoch)\n",
    "    print(f'Epoch {epoch} done')\n",
    "    if loss.item() < MAX_LOSS:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "92565cc2-618d-4769-baf0-511e9b6f4a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "dd9fb40f-c281-4f5e-813e-b2939c519c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.9306565382528487\n",
      "F1 Score (Micro) = 0.9369202226345084\n",
      "F1 Score (Macro) = 0.8762412181303967\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    outputs, targets = validation(epoch)\n",
    "    outputs = np.array(outputs) >= 0.5\n",
    "    accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "    print(f\"Accuracy Score = {accuracy}\")\n",
    "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "    print(f\"F1 Score (Macro) = {f1_score_macro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "d813fc83-87a5-4a71-99cb-11bc108d874c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.3982105255126953, 1.8183958530426025, -2.7816433906555176, -2.939051628112793, -2.5525059700012207, -2.1630332469940186]\n"
     ]
    }
   ],
   "source": [
    "# Test our inputs\n",
    "# Choose a sentence\n",
    "sample_row = new_df.sample()[['sentence', 'label']]\n",
    "sample_text = sample_row['sentence'].values[0]\n",
    "\n",
    "def get_dimension_from_prediction(v, dict=dimension_dictionary):\n",
    "    dims = [dim[\"name\"] for dim in dimension_dictionary]    \n",
    "    index = v.index(max(v))    \n",
    "    return dims[index]\n",
    "    \n",
    "# Encode the sentence\n",
    "encoding = tokenizer.encode_plus(\n",
    "  sample_text.lower(),\n",
    "  add_special_tokens=True,\n",
    "  max_length=MAX_LEN,\n",
    "  padding='max_length',\n",
    "  truncation=True,\n",
    "  return_token_type_ids=True,\n",
    "  return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids = encoding[\"input_ids\"]\n",
    "mask = encoding[\"attention_mask\"]\n",
    "token_type_ids = encoding[\"token_type_ids\"]\n",
    "input_ids = input_ids.to(device, dtype=torch.long)\n",
    "mask = mask.to(device, dtype=torch.long)\n",
    "token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "\n",
    "# Inference\n",
    "output = model(input_ids, mask, token_type_ids)\n",
    "predictions = output[0].tolist()\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "e46c0102-e990-4da2-b6a2-1297260c77ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statment \"Because of some limited repression against the political opposition and pro-democracy NGOs, the protests of 2011 and 2012 were not repeated in 2016.\" was classified as: participatory.\n"
     ]
    }
   ],
   "source": [
    "result_dim = get_dimension_from_prediction(predictions)\n",
    "print(f'Statment \"{sample_text}\" was classified as: {result_dim}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "87279a44-baf1-4913-b5d1-05eeb8471fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model, 'BERT_classifier_democracy.pth')\n",
    "torch.save(model.state_dict(), 'BERT_classifier_democracy_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdeb72e-899d-4cd8-bd94-e3938b616517",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
