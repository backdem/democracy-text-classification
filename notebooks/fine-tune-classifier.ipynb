{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "da8588ef-60db-4f9c-a6de-e9f63d03129a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "e868df8d-0e2b-45d9-bf04-e88c8e98cd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(\"Using device: \" + device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "2edcfaa5-02a9-4cc4-b35f-e2ae6e2bc180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found topics: ['electoral', 'participatory', 'media', 'liberal_institution', 'liberal_rights', 'no_dimension']\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Corpus and dictionary files to use\n",
    "corpus_file = 'democracy_reports_corpus.csv'\n",
    "dictionary_file = 'dimension_dictionary.json'\n",
    "corpus_file_url = \"https://github.com/backdem/democracy-datasets/raw/main/democracy_reports_corpus.csv\"\n",
    "dictionary_file_url = \"https://raw.githubusercontent.com/backdem/democracy-datasets/main/dimension_dictionary.json\"\n",
    "\n",
    "# Download datsets if not already downloaded\n",
    "if not os.path.exists(corpus_file):\n",
    "    urllib.request.urlretrieve(corpus_file_url, corpus_file)\n",
    "if not os.path.exists(dictionary_file):\n",
    "    urllib.request.urlretrieve(dictionary_file_url, dictionary_file)\n",
    "\n",
    "def load_json_dict(dict_file):\n",
    "    with open(dict_file, 'r') as file:\n",
    "        dictionary = json.load(file)\n",
    "        dictionary.append({\n",
    "            'name': 'no_dimension',\n",
    "            'words': []\n",
    "        })\n",
    "        return dictionary\n",
    "\n",
    "dimension_dictionary = load_json_dict(dictionary_file)\n",
    "print(f'Found topics: {[dim[\"name\"] for dim in dimension_dictionary]}')\n",
    "# Naive approach to label sentences with dictionary, producing \n",
    "# a masked label vector of the form [0, 0, 0, 0, 0, 1] where the \n",
    "# indicies match topics ['electoral', 'participatory', 'media', 'liberal_institution', 'liberal_rights', 'no_dimension']\n",
    "def generate_label_vector(sentence, dict=dimension_dictionary):\n",
    "    topics = [dim['name'] for dim in dict]\n",
    "    matched_dim = 'no_dimension'\n",
    "    for dim in dict:\n",
    "        if matched_dim != 'no_dimension':\n",
    "            break\n",
    "        for w in dim['words']:\n",
    "            if w in sentence.lower():\n",
    "                matched_dim = dim['name']\n",
    "                break\n",
    "    return [int(t == matched_dim) for t in topics]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "44882512-1124-4aaa-9ee5-06306211f567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 sentence  \\\n",
      "218878  greco recommended to ensure that the relevant ...   \n",
      "62974   However, the authorities regularly use violenc...   \n",
      "129469  In addition, existing institutions are undereq...   \n",
      "355742  such advertisements must contain no informatio...   \n",
      "317181  greco takes note of the information provided a...   \n",
      "\n",
      "                                                  section             country  \\\n",
      "218878                                               none            slovenia   \n",
      "62974   ['Executive Summary', 'Score changes in 2015',...          azerbaijan   \n",
      "129469                                                NaN  bosnia-herzegovina   \n",
      "355742                                               none              france   \n",
      "317181                                               none              poland   \n",
      "\n",
      "        year                        source  \n",
      "218878  2009                         greco  \n",
      "62974   2015  freedomhouse_nations-transit  \n",
      "129469  2008                           bti  \n",
      "355742  2008                         greco  \n",
      "317181  2010                         greco  \n"
     ]
    }
   ],
   "source": [
    "# Read csv file into Dataframe\n",
    "df = pd.read_csv(corpus_file, dtype={'year': str, 'sentence': str}, comment='#')\n",
    "# Print first row\n",
    "print(df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "ba308218-7f5b-4c6c-b0fb-07cf5f305555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label vector column\n",
    "df_sentences_labels = pd.DataFrame(df['sentence'])\n",
    "df_sentences_labels['label'] = df_sentences_labels['sentence'].apply(generate_label_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "b06f297b-13c1-4775-b201-bc4b0d92d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "FRACTION_OF_DS_TO_USE = 0.1\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "MAX_LOSS = 0.02\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "d8340b0d-717f-4135-a422-a0d9d8e83059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15233</th>\n",
       "      <td>Alleging electoral fraud, the opposition rejec...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207408</th>\n",
       "      <td>During the period under review, the system of ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372364</th>\n",
       "      <td>by way of example, a former head of the centra...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194777</th>\n",
       "      <td>In 1919, after Word War I, a mandate for the r...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6319</th>\n",
       "      <td>The Association of Free Trade Unions of Sloven...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327667</th>\n",
       "      <td>by contrast, no classes on these issues have s...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157529</th>\n",
       "      <td>The functioning and management of public educa...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40138</th>\n",
       "      <td>Although not holding a formal government posit...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180149</th>\n",
       "      <td>Please cite as follows: Bertelsmann Stiftung, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431295</th>\n",
       "      <td>to unambiguously cover bribery of foreign arbi...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence               label\n",
       "15233   Alleging electoral fraud, the opposition rejec...  [1, 0, 0, 0, 0, 0]\n",
       "207408  During the period under review, the system of ...  [0, 0, 0, 0, 0, 1]\n",
       "372364  by way of example, a former head of the centra...  [0, 1, 0, 0, 0, 0]\n",
       "194777  In 1919, after Word War I, a mandate for the r...  [1, 0, 0, 0, 0, 0]\n",
       "6319    The Association of Free Trade Unions of Sloven...  [0, 1, 0, 0, 0, 0]\n",
       "327667  by contrast, no classes on these issues have s...  [0, 0, 0, 1, 0, 0]\n",
       "157529  The functioning and management of public educa...  [0, 0, 0, 0, 0, 1]\n",
       "40138   Although not holding a formal government posit...  [0, 0, 0, 0, 0, 1]\n",
       "180149  Please cite as follows: Bertelsmann Stiftung, ...  [0, 0, 0, 0, 0, 1]\n",
       "431295  to unambiguously cover bribery of foreign arbi...  [0, 0, 0, 0, 0, 1]"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use part of the dataset\n",
    "new_df = df_sentences_labels.sample(frac=FRACTION_OF_DS_TO_USE, random_state=200)\n",
    "# Display a sample of the dataset\n",
    "new_df.sample(10)[['sentence', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "caf982b0-ceba-4f6a-b82a-de77aa727c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.comment_text = dataframe.sentence\n",
    "        self.targets = self.data.label\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comment_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment_text = str(self.comment_text[index])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "aba08395-2d99-4a90-86e2-b9ae7126776c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (46073, 2)\n",
      "TRAIN Dataset: (36858, 2)\n",
      "TEST Dataset: (9215, 2)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_dataset=new_df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=new_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "56ef5fc2-7e42-492f-b784-60af2502b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "63817466-1fa8-4374-98ae-754830ef222d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l2): Dropout(p=0.3, inplace=False)\n",
       "  (l3): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 6)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "976033ad-b99d-4064-afd8-c8b1735fff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "e4d1e80e-f264-49e5-9246-39c497e48c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "2a6fc239-7a30-447e-8aca-fe460c4ae3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        if _%500==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        if loss.item() < MAX_LOSS:\n",
    "            break\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "23bc7deb-ef71-4496-a8ae-49419949da33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.6917052268981934\n",
      "Epoch: 0, Loss:  0.13105759024620056\n",
      "Epoch: 0, Loss:  0.10825284570455551\n",
      "Epoch 0 done\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for epoch in range(EPOCHS):\n",
    "    loss = train(epoch)\n",
    "    print(f'Epoch {epoch} done')\n",
    "    if loss.item() < MAX_LOSS:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "92565cc2-618d-4769-baf0-511e9b6f4a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "dd9fb40f-c281-4f5e-813e-b2939c519c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.9306565382528487\n",
      "F1 Score (Micro) = 0.9369202226345084\n",
      "F1 Score (Macro) = 0.8762412181303967\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    outputs, targets = validation(epoch)\n",
    "    outputs = np.array(outputs) >= 0.5\n",
    "    accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "    print(f\"Accuracy Score = {accuracy}\")\n",
    "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "    print(f\"F1 Score (Macro) = {f1_score_macro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "d813fc83-87a5-4a71-99cb-11bc108d874c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.804351568222046, -2.9989209175109863, -3.4676501750946045, -3.5935776233673096, -3.6320676803588867, -2.935041904449463]\n"
     ]
    }
   ],
   "source": [
    "# Test our inputs\n",
    "# Choose a sentence\n",
    "sample_row = new_df.sample()[['sentence', 'label']]\n",
    "sample_text = sample_row['sentence'].values[0]\n",
    "\n",
    "# Encode the sentence\n",
    "encoding = tokenizer.encode_plus(\n",
    "  sample_text.lower(),\n",
    "  add_special_tokens=True,\n",
    "  max_length=MAX_LEN,\n",
    "  padding='max_length',\n",
    "  truncation=True,\n",
    "  return_token_type_ids=True,\n",
    "  return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids = encoding[\"input_ids\"]\n",
    "mask = encoding[\"attention_mask\"]\n",
    "token_type_ids = encoding[\"token_type_ids\"]\n",
    "input_ids = input_ids.to(device, dtype=torch.long)\n",
    "mask = mask.to(device, dtype=torch.long)\n",
    "token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "\n",
    "# Inference\n",
    "output = model(input_ids, mask, token_type_ids)\n",
    "predictions = output[0].tolist()\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "e46c0102-e990-4da2-b6a2-1297260c77ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dimension_from_prediction(v, dict=dimension_dictionary):\n",
    "    dims = [dim[\"name\"] for dim in dimension_dictionary]\n",
    "    max = 0\n",
    "    index = -1\n",
    "    for i in range(len(v)):\n",
    "        if v[i] > max:\n",
    "            max = v[i]\n",
    "            index = i\n",
    "    return dims[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "c8e250a5-aa00-4934-afea-856d70f39413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statment \"all statements were, however, submitted in the 2009 elections to the european parliament.\" was classified as: electoral.\n"
     ]
    }
   ],
   "source": [
    "result_dim = get_dimension_from_prediction(predictions)\n",
    "print(f'Statment \"{sample_text}\" was classified as: {result_dim}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fdcb95-2b1d-4f7b-adf7-8ac8e7c344cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
